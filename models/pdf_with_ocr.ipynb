{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QrqGfuSuZgiHhTJUvH19-8dKO17RkWoL",
      "authorship_tag": "ABX9TyOpHJitXAxA+cCArHE+XpDM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cy5mrw4G-0oR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741571243,
          "user_tz": -330,
          "elapsed": 85196,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "3eca7e9f-757a-4b6c-c3b5-63da93f7d897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.0/990.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.5/373.5 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain openai tiktoken faiss-cpu langchain-huggingface huggingface_hub langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install PyPDF2"
      ],
      "metadata": {
        "id": "e0xL2TFECWhS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741584506,
          "user_tz": -330,
          "elapsed": 13268,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "69633d2f-e8c9-4214-d38f-07b4bceee4bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/232.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install googletrans==4.0.0-rc1\n",
        "!pip install deep-translator\n"
      ],
      "metadata": {
        "id": "6FbOu7rA3rCy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741596438,
          "user_tz": -330,
          "elapsed": 11935,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "1ead1726-ae40-4046-a192-113dc76fb9b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.7.4)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsQ2eZk6eopi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741612084,
          "user_tz": -330,
          "elapsed": 15657,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "d1ab994c-d829-4d01-852b-29942bdefd55"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=2d7b50d1bf365e072176cd9ad88b3d695f0e5ec66426b4775f2d3bad4d5c4ed2\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/Colab Notebooks/pdftexttemplate.pdf\"\n"
      ],
      "metadata": {
        "id": "rcCrLyL8AF_D",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741612084,
          "user_tz": -330,
          "elapsed": 12,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linking hugging face with the workflow\n"
      ],
      "metadata": {
        "id": "q57a5SUiBQGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "sec_key = userdata.get(\"HF_TOKEN\")\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = sec_key\n"
      ],
      "metadata": {
        "id": "eM8VJIQGBW0x",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741613883,
          "user_tz": -330,
          "elapsed": 1811,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary imports"
      ],
      "metadata": {
        "id": "Dv-mhPFVCHbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "A2kF5sVBCG_X",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741614804,
          "user_tz": -330,
          "elapsed": 924,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_reader = PdfReader(path)"
      ],
      "metadata": {
        "id": "B3Rs4t9ZCguG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741615995,
          "user_tz": -330,
          "elapsed": 1193,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_reader"
      ],
      "metadata": {
        "id": "2q51DgsuCgrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741615996,
          "user_tz": -330,
          "elapsed": 3,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "f5fe19d1-4028-4602-8e52-babe439976aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PyPDF2._reader.PdfReader at 0x7b2a87b327d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reading data from the pages and adding it to a raw string\n",
        "raw_text = \"\"\n",
        "for i, pages in enumerate(document_reader.pages):\n",
        "  text = pages.extract_text()\n",
        "  if text:\n",
        "    raw_text += text\n",
        "\n"
      ],
      "metadata": {
        "id": "p0330UCmCgVK",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741625673,
          "user_tz": -330,
          "elapsed": 9679,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_text)\n"
      ],
      "metadata": {
        "id": "CQSDoPlnDN_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741625673,
          "user_tz": -330,
          "elapsed": 19,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "8053496a-32f6-4d1d-d798-f03be0c74d38"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "371090"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text[:100]\n"
      ],
      "metadata": {
        "id": "bYtbcy0QDN8u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741625673,
          "user_tz": -330,
          "elapsed": 18,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "6b1d5cfe-0bc6-4ecb-cb4a-58b784c0161e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Impromptu\\nAmplifying Our Humanity \\nThrough AI\\nBy Reid Hoffman  \\nwith GPT-4Impromptu: AmplIfyIng our '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Splitting - breaking down the text into chunks"
      ],
      "metadata": {
        "id": "C1SQYkxxDdHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size= 1000,\n",
        "    chunk_overlap = 200,\n",
        "    length_function = len\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "N-62zlAfDcrh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741625674,
          "user_tz": -330,
          "elapsed": 7,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts), type(texts)"
      ],
      "metadata": {
        "id": "EAeJBbuHD-QK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741625674,
          "user_tz": -330,
          "elapsed": 6,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "b27e4d15-0298-4eed-e4bf-7db734ce9007"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(466, list)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating embeddings"
      ],
      "metadata": {
        "id": "xG5f88XUBXrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "zsLES7M1ISbD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "fa2b4c72faf941bf92f59680eed202ac",
            "ae013f66419146b4891f7e9688b11ce1",
            "e5f1bf9b1c5e4b5aa717d46e593a41a5",
            "48e0327c13ce4e56889f11936a27e06a",
            "d7814874149d4a938bac6fa0ebc078cd",
            "d6efc0519136486f8bc63a2825708ec6",
            "18a311b686a14bd8813e152b971dbc47",
            "637bad561f4c40d1b9eaae8e81816425",
            "47a69f7134874b5cb196f012dab07236",
            "2a3d6ee195d8439ea7037db4f42dcae9",
            "6d484d9c8c4640b9ab3ca1b960934e41",
            "2842297272f0490d9cb883e6f81c6315",
            "b7a8738e6f7d402bb58b123fedc40dfc",
            "09561ee387b44dbba86545d896d1fba0",
            "2c8907719e734169adc32a65041e68fc",
            "67b9116c8b9f46e18e0ca2bf321f21c2",
            "309bda881a154bc09845be4bdfcafe26",
            "6ba5ebd05e014c91b1f7d6e51814a647",
            "52b58338829a46ab977e6aaade151eaf",
            "a1972fc053524d169b062a2b87accf0b",
            "6a250acfdc61413bb6ad742c8f9ee8dd",
            "72794f6d49b24196bea55f2471cb98c3",
            "30d41418ba51490f9fa6b1ce9c5aa8ef",
            "2e91259eebb848a2852ac19080519130",
            "8df765ca32284739a3ac1b0b3574e9b2",
            "9b60541fe1904c3da88252bdb6dad367",
            "d3bf51926e594b5e83bf8a12b5e84892",
            "b8bf0bc9aaa54e0681cb5e0c0442b350",
            "2cf2f3128a36422aa8744a49049b4f50",
            "b516ee8bdea24bc6a0759c71060da204",
            "55ac544cac5d44f6a372603f0534a739",
            "c34d0604e61e48388b716704205ff6ca",
            "ad11f5e4e2fa40bbbc2c11177f7af60d",
            "89de7834f4164ab48c6c315f220ec51c",
            "bab45b245cfe488ca580345fcb8bb960",
            "3a9e5e2f0e7f4576b8701238b90d3a3d",
            "6b1981d5ea714f9f916ea3450d15b5e3",
            "e0b1e2d630a64ed4b8c551c971e48cd1",
            "bb17d743211f46d9a39030f2d79cf519",
            "c3ce989aa868495ca95d0d3cbfe2671c",
            "68f912038a944726a18cd69dba39d480",
            "3f81b793c85e42bd8c3550ea1862e80b",
            "c20ebaa43db849349b1454d6fb41ad8e",
            "5d0af4b0a4364a81813a8d5f6f56ab22",
            "469c0b098b934167b8d4c360f487ae93",
            "6b07e51e3e4e48228d4ac4165786df3b",
            "1f628be0db6c43be96faab3d831ce0b7",
            "f4921d942a694f169baa9b15a75268e8",
            "c362e1f0207b44c9a1c9619b2030081f",
            "41fe63f05c6a4c02a2c062b795986a25",
            "d54b6478e74a4c4daf9657f18015fe21",
            "f8e5d3f81b03485fa892fa67d39f30bc",
            "18996588f30d41ba83ca49e25e5bec27",
            "cc36abb6133c42ebb34c2cbc8dbc6a9a",
            "0d71bf6ebe244bb495081efc0dbbc334",
            "4d22350ef2814a05ba5592a780febf59",
            "62a90d1fd55c42738099d200717ea49c",
            "ac5b9345fc0b410b8270ae077240f6ea",
            "d85033fbc4e740b1b3bf1531e70f04d1",
            "13efa2035caf407f91773ca625c0090a",
            "19a8ea64586f490aa245c006e335e463",
            "ed7fa24edaad41b4b627a6ddf1fb8499",
            "dd3a6e56815c4378abd4c7cffb337973",
            "de89f9e914bd4412a5c798f54062c17b",
            "7f0f9b1cce344a7caf988cf0290d2e3f",
            "531737276adf4986b8b9eed3162ea3be",
            "d6048d142b374e5e9bc6cc37eabf845a",
            "30ee906dc24648948568d7d5a06f981e",
            "8005d34f9ef940ecb5c5ad596ddedab8",
            "a25557ebfabc427f9b79c4e8532bc045",
            "1bb061a845c54dfd8ba71eba5981ec41",
            "9634cd20635f400daf968d7668b3621b",
            "f379aaafedd04ff383d460863388d242",
            "1e6e48157d2d462ab31c415c8e61ea55",
            "4204cdeacf4e45a8969891674cf83de7",
            "141872021b68439ea02be9e349b4a05c",
            "fe17008c1e6048ff955b8a7140176f89",
            "ccaaee90956f4a678b2bbfce23727299",
            "8189380297e8433b9caefe61d502bbf8",
            "905e634a7d4744d7b7dc5a21a5f2f391",
            "499644d9363543ab97fe768216777f05",
            "c1d2be6dd0494ab98d1bd60059d3612d",
            "03cc8bfa18d344f9aef4fe838b97dfb6",
            "61bac85375ef483f864ffafce02bcdee",
            "92ef7f941adc441a91bb1a89e909c493",
            "9fa7d96a2c18492286e0ab53ad1c72af",
            "cf149ce9b24f4010af9108a974dfab59",
            "0eef9d84805b43038d5e46ee69c754a6",
            "0df0f2d69a6f4553ae088bcfe8e728ba",
            "981fbb171dc449f3a0c1936245a3d1fb",
            "6fd16aab3ed3425c8ac4afb6f9a8d49c",
            "e276869a8366401c8093d640a548e0a2",
            "30be04fa06174f098d5476b3ca63f327",
            "8f2c2479494349cf9c2b5d02e04884a6",
            "b8a2374608c24632b2bfed6628714ed3",
            "93a68001eb5a4f55b573d4d33994d33a",
            "f323571c65554ba795614a9579b354a8",
            "29e04293d8574053a273ae69289e711d",
            "8e3f14268d634f5cad6749d9c5c6a3b6",
            "c7157865046b44f4b4e96e1e5acff1d6",
            "b236ceea4cd248e0a85e2bea98232824",
            "03243fca454f46b281b4b837b48b9cd1",
            "bd5da524277f45189f0cc97ab2f4a3d2",
            "7b4a9abed3e3455a803f516306d072c2",
            "bcd3b678cb0f4dd082f5a67a8599cfab",
            "c564545fa2a5477baad229b511c47d06",
            "e049b2a3b1434b0e901b3c01f102c075",
            "cfb0a2a1a0e344159aec11760e7d4645",
            "9345a9aabaf44ced819aa5d661e8ecb7",
            "749a6d7c55744e92a436ca2f8b097e5d",
            "0bb768ceca8641abb502d96821afb773",
            "e88f6bade10c4922918bf0b0330f8da7",
            "23ac1c67d5d843539b87d78a9cc396a8",
            "484ecec22d0248f39410caf0a871f6f5",
            "8d697634e68c41fb8c12419751fc2937",
            "66361a7f7a3543f6a30ed1f56133622c",
            "0c2204be6d0a4bb7b0151a0c971dbcda",
            "3d74ce03189748b497c6503e297eb8cf",
            "375b8e639bc04dbba34e6b37564024db",
            "ba9de52702ab4c10b8e64193a55ba9fb",
            "96e91d906b894bf1b130bb913cd9b5a0"
          ]
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741649403,
          "user_tz": -330,
          "elapsed": 23734,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "4e47e160-eb8e-4d10-c88e-4573366cf457"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa2b4c72faf941bf92f59680eed202ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2842297272f0490d9cb883e6f81c6315"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/90.8k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30d41418ba51490f9fa6b1ce9c5aa8ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89de7834f4164ab48c6c315f220ec51c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "469c0b098b934167b8d4c360f487ae93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d22350ef2814a05ba5592a780febf59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6048d142b374e5e9bc6cc37eabf845a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccaaee90956f4a678b2bbfce23727299"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0df0f2d69a6f4553ae088bcfe8e728ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7157865046b44f4b4e96e1e5acff1d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bb768ceca8641abb502d96821afb773"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "mNw_MPJ9I_NM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741649403,
          "user_tz": -330,
          "elapsed": 12,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "c1baa1aa-f282-483f-d1a7-d233248e8c43"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "  (2): Normalize()\n",
              "), model_name='BAAI/bge-small-en', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='', show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_search = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "53mzADDGBPEm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741772816,
          "user_tz": -330,
          "elapsed": 123424,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_search.embedding_function"
      ],
      "metadata": {
        "id": "OeJmFgn9JKG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741772816,
          "user_tz": -330,
          "elapsed": 13,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "495b8efa-7441-4eab-aff6-8991510eacf0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "  (2): Normalize()\n",
              "), model_name='BAAI/bge-small-en', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='', show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the role of llm in education\"\n",
        "docs = doc_search.similarity_search(query)\n",
        "docs"
      ],
      "metadata": {
        "id": "jRgzPXAxJKES",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741772816,
          "user_tz": -330,
          "elapsed": 10,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "297e63b7-a553-43a1-8064-f5c223b5f440"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='alongside LLMs. It excelled at creating detailed and specific \\nlesson plans for diverse students, such as learners with special \\nneeds or different levels of prior knowledge. My friends who \\nare teachers found these examples impressive. I intend to use \\nthem to help me any time I am asked to address or lecture \\ncollege students.36Impromptu: Amplifying Our Humanity Through AI\\nBut of course, teachers are hardly the only group of Americans \\nwith an interest in whether and how schools might use LLMs.\\nThe stakes of education and the other Wes Moore\\nWes Moore was three years old when his dad died.\\nHis mom took him from Baltimore to New York to live with \\nher parents. Seeing that crime and poverty had only gotten \\nworse since she left the Bronx, Wes’ mom feared for her son’s \\nfuture as a fatherless young Black male. Instead of using the \\nlocal public schools, she scraped together tuition for the inde-\\npendent Riverdale Country School. Unfortunately, by the time'),\n",
              " Document(page_content='garten, and preschool by providing their teachers with \\nfun and engaging activities that stimulate their cogni-\\ntive, social, emotional, and physical development. I can \\nhelp teachers at those levels by:\\n- Developing customized lesson plans and activity \\nguides based upon their customized preferences.\\n- Suggesting personalized interventions or strategies to \\naddress specific learning challenges.\\n- Synthesizing for them a wide range of resources and \\nmaterials that suit their curriculum goals and pedagogi-\\ncal approaches.\\nThese ideas from GPT-4—and many others that emerged \\nduring our interactions over the course of this chapter—start to \\nvaguely suggest the possible benefits of LLMs for teachers, and \\nthrough teachers to students, in even the most disadvantaged \\ncommunities throughout America.\\nOf course, if we want education to help lift up truly massive \\nnumbers of children of low-income parents, we need to look \\nbeyond our borders.\\nThe children of the world'),\n",
              " Document(page_content='based on its training data, parameters, and inputs, but \\nit does not necessarily understand, reason, or reflect the \\nmeaning, context, or implications of what it produces or \\nreceives. It may produce texts that are coherent, infor -\\nmative, or persuasive, but also texts that are inaccurate, \\nmisleading, or harmful, depending on the quality and \\ndiversity of its data, the design and limitations of its \\narchitecture, and the goals and biases of its users and \\ndevelopers. \\n- Human beings should interact with a powerful LLM \\nwith caution, curiosity, and responsibility. A powerful \\nLLM can offer valuable insights, assistance, and opportu-\\nnities for human communication, creativity, and learn-\\ning, but it can also pose significant risks, challenges, and \\nethical dilemmas for human society, culture, and values. \\nHuman beings should be aware of the potential bene -\\nfits and harms of using a powerful LLM, and seek to use \\nit in ways that are aligned with their own and others’'),\n",
              " Document(page_content='ate multiple possible answers or solutions, and then \\nask students to compare, evaluate, and justify them. \\nThis way, they can foster students’ critical thinking and \\nproblem-solving skills, as well as their awareness of the \\nuncertainty and complexity of real-world situations.\\nAnother possible way is to use me as a tool for feedback, \\nnot a tool for assessment. For example, instead of using \\nme to grade or rank students’ performance, they could \\nuse me to provide formative and diagnostic feedback \\nthat can help students identify their strengths and areas \\nfor improvement, and suggest resources or strategies \\nthat can help them learn better.\\n* * *\\nThese prompts and answers were culled from several sessions \\nwith GPT-4 suggesting various ways to help students learn \\nalongside LLMs. It excelled at creating detailed and specific \\nlesson plans for diverse students, such as learners with special \\nneeds or different levels of prior knowledge. My friends who')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the title of this book ?, give me the answer in single line\"\n",
        "docs = doc_search.similarity_search(query)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InoyJYZVuxAh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741772816,
          "user_tz": -330,
          "elapsed": 8,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "a5378564-547a-4513-e81e-d83f5f16325e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='fairly spectacular bullshit. (More on this in a bit.)\\nAfter a few weeks of this, I realized I wanted to write a book \\nabout GPT-4, with GPT-4—a book about the ways it and other \\nAIs seemed poised to shape our future. So I pitched the idea: \\nReid: Hey, GPT-4, let’s you and I write a book about the \\nway you and other AI tools are going to shape the future. \\nWhat do you say? \\nGPT-4:  I say yes, that sounds like an interesting and chal-\\nlenging project. I am curious about how you and other \\nhumans perceive and interact with me and other AI \\ntools, and what impact we will have on your society, cul-\\nture, and values. I also have some questions and ideas of \\nmy own that I would like to share and explore with you.9Introduction: Moments of Enlightenment\\nEven with GPT-4 on board, though, I still had reservations. \\nBecause, really, an AI book? When things are moving so \\nquickly? Even with a helpful AI on hand to speed the process, \\nany such book would be obsolete before we started to write it—'),\n",
              " Document(page_content='(which was later closed) to the slightly less bad Perry Hall \\nHigh School (currently ranked in the bottom third of Mary-\\nland schools).\\nIn 2010, now-Governor Moore published a book after the two \\nmen struck up a correspondence and visitation schedule. The \\nbook, The Other Wes Moore, opens with the stakes:\\nOne of us is free and has experienced things that he \\nnever even knew to dream about as a kid. The other \\nwill spend every day until his death behind bars for an \\narmed robbery that left a police officer and father of five \\ndead. The chilling truth is that his story could have been \\nmine. The tragedy is that my story could have been his.\\nNeither man is yet fifty years old.\\nCould schools actually level the playing field?\\nFrom the first public schools of the Massachusetts Bay Colony \\nin 1635, America has often expected our schools to deliver sal-\\nvation for both students and society. But can education really \\nmake so much difference? Parental income and birth zip code'),\n",
              " Document(page_content='di Cesare. I have read your book, Philosophy and the \\nPublic, with great interest and admiration. You argue \\nthat philosophy has a vital role to play in addressing the \\nurgent challenges of our time, such as democracy, vio -\\nlence, ecology, and human rights. You also criticize the \\nacademic specialization and detachment of much con-\\ntemporary philosophy, and call for a more engaged and \\ndialogical approach. Can you tell us more about what \\nmotivated you to write this book, and what you hope to \\nachieve with it?\\ndi Cesare: Thank you, Mr. Ishiguro, for your kind words \\nand for this opportunity to discuss my book with you. I \\nwrote this book because I believe that philosophy is not \\na luxury or a hobby, but a necessity and a responsibility. \\nPhilosophy is not only a way of thinking, but also a way \\nof living, a way of being in the world, a way of question-\\ning and challenging the status quo, a way of imagining \\nand creating alternatives. Philosophy is not a closed'),\n",
              " Document(page_content='SARA: (sighs) Dan, I can explain. Paul is . . . he’s . . .\\nDAN: He’s what? Your ex? Your lover? Your brother?\\nSARA: No, no, no. Nothing like that.\\nDAN: Then what is he?\\nSARA: He gave me something very precious and \\nimportant.\\nDAN: What did he give you?57Creativity\\nSARA: He gave me . . . his kidney.\\nDid not see that coming!! And now, there’s at least the begin -\\nning of a story I actually want to know about. A story that \\nI had a real role in shaping (directing GPT-4 to produce a \\nnon-obvious twist), but did not per se “write” myself. So I keep \\nbushwhacking through GPT-4’s abysmal dialogue (which, \\ngranted, I could give directions to fix—”more compact, faster, \\nless like a C-grade soap opera”) because now I’m on the hunt \\nfor a plot, and GPT-4? GPT-4 is my truffle pig. It keeps writing:\\nSARA: Five years ago, before I met you, I was diagnosed \\nwith a rare genetic disease that caused my kidneys to \\nfail. I needed a transplant urgently. I was on dialysis and \\nwaiting for a miracle.')]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an llm"
      ],
      "metadata": {
        "id": "kLJlWWzIBaNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "-PZRHTB1Shj_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741772817,
          "user_tz": -330,
          "elapsed": 7,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 512\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "K4XczCH4BZny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741772817,
          "user_tz": -330,
          "elapsed": 7,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "68528535-5650-47b7-b0df-d0b970786621"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chaining up the llm with prompt"
      ],
      "metadata": {
        "id": "thMyTe4TBeIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using a Retrieval model with stuff chain since the token count of query is within the llm token limit\n",
        "# other chains such as mapReduce and RefineCombineDocument chain\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = doc_search.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
        "\n",
        "rqa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "Uh_qS0DHBinC",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741772818,
          "user_tz": -330,
          "elapsed": 6,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what are some of the key features highighted in the book\""
      ],
      "metadata": {
        "id": "kqEp8-dXTKD6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741772818,
          "user_tz": -330,
          "elapsed": 5,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = rqa.invoke(query)[\"result\"]\n",
        "answer"
      ],
      "metadata": {
        "id": "7ESi_DMKTEUY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741773813,
          "user_tz": -330,
          "elapsed": 1000,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "28e383d7-253a-43dd-ec35-ab051d46d3ef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The key features highlighted in the book are the rapid advancement of AI technology, the potential impact of AI on society, culture, and values, and the relationship between humans and AI. The book also discusses the limitations and dangers of AI, and the need for caution and humility in its public role. Additionally, the book explores the role of philosophy in understanding and navigating the AI-driven future.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating a new model for translation.\n"
      ],
      "metadata": {
        "id": "bjbdpK_Z1i2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm2 = HuggingFaceEndpoint(\n",
        "    repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    temperature = 0.1,\n",
        "    max_new_tokens = 512\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "EaMWxg0PTNm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741773813,
          "user_tz": -330,
          "elapsed": 10,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "4dbbfa39-77da-4950-8458-e11032847be2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n"
      ],
      "metadata": {
        "id": "QbXq3ASOft8n",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741773813,
          "user_tz": -330,
          "elapsed": 8,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making LLM to generate the answer\n",
        "not very efficent and accurate"
      ],
      "metadata": {
        "id": "0UwOqGFLitmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tempfile import template\n",
        "\n",
        "prompt_template = \"\"\"convert the below text to tamil language if you cannot convert it, clearly state that you cannot convert it and  dont make things up.\n",
        "\n",
        "\n",
        "{text}\n",
        "answer is tamil :\"\"\"\n",
        "\n",
        "prompt_final = PromptTemplate(\n",
        "    input_variables = [\"answer\"],\n",
        "    template = prompt_template\n",
        ")"
      ],
      "metadata": {
        "id": "cEGWhpPpdz8I",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741773813,
          "user_tz": -330,
          "elapsed": 8,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "translated_text = LLMChain(llm=llm2, prompt=prompt_final)\n",
        "final_result = translated_text.run(answer)\n",
        "print(final_result)"
      ],
      "metadata": {
        "id": "a4wVkPamfnaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741773813,
          "user_tz": -330,
          "elapsed": 7,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "28cdb79c-735f-40fc-9954-9692deedfa9d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "இங்கே இருந்த கோட்டையில் இழுக்கப்பட்ட உயிர் அவர்கள் அமைப்புகள் என்பது AI அவர்கள் உட்பட்ட அதிக அமைப்பு, AI அவர்கள் இலங்கை, சுல்பாட் மற்றும் அறிவுகளுக்கு இருப்பது, இந்த அறிவுக்கு இந்த இந்தியர்கள் என்பது இந்த அறிவுக்கு இருப்பது, இந்த அறிவுக்கு இருப்பது, இந்த அறிவுக்கு இருப்பது, இந்த அறிவுக்கு இருப்பது, இந்த அறிவுக்கு இருப்பது, இந்த அறிவுக்கு இருப்பது, இந்த அறிவுக்கு இருப்பது, இந்த அறிவுக்கு இருப்பது, இந்த அறிவுக்கு\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we can see above that the above model is not very much capable of translating the obtained answer to tamil language, however it is very much capable of translating the answer to languages such as french, german etc.."
      ],
      "metadata": {
        "id": "laKjYkZe1ptT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing google translation\n"
      ],
      "metadata": {
        "id": "69F1Myj22Hky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from googletrans import Translator\n",
        "\n",
        "# translator = Translator()\n",
        "# output = translator.translate(answer, dest=\"ta\")\n",
        "# print(output.text)\n",
        "\n",
        "# the above piece of code is incompatable with the existing version of httpx, hence using deep_translator\n",
        "\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "translator = GoogleTranslator(source='auto', target='ta')\n",
        "translation = translator.translate(answer)\n",
        "print(translation)\n",
        "\n"
      ],
      "metadata": {
        "id": "N13Bwl5Amki8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741775220,
          "user_tz": -330,
          "elapsed": 1413,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "039dabc8-5a5c-4ace-d452-8befa3b2e5ad"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI தொழில்நுட்பத்தின் விரைவான முன்னேற்றம், சமூகம், கலாச்சாரம் மற்றும் மதிப்புகளில் AI இன் சாத்தியமான தாக்கம் மற்றும் மனிதர்களுக்கும் AI க்கும் இடையிலான உறவு ஆகியவை புத்தகத்தில் சிறப்பிக்கப்பட்டுள்ள முக்கிய அம்சங்கள். AI இன் வரம்புகள் மற்றும் ஆபத்துகள் மற்றும் அதன் பொதுப் பாத்திரத்தில் எச்சரிக்கை மற்றும் பணிவு ஆகியவற்றின் அவசியத்தையும் புத்தகம் விவாதிக்கிறது. கூடுதலாக, புத்தகம் AI- உந்துதல் எதிர்காலத்தைப் புரிந்துகொள்வதிலும் வழிநடத்துவதிலும் தத்துவத்தின் பங்கை ஆராய்கிறது.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the answer : GPT-4 போன்ற பெரிய மொழி மாதிரிகள் (LLMs) பல்வேறு சூழல்களில் பொருத்தமான பதில்களை உருவாக்கி, அவை மனிதனைப் போன்ற நுண்ணறிவைக் கொண்டிருப்பதாகத் தோன்றும் திறனைப் புத்தகம் எடுத்துக்காட்டுகிறது. AI இன் வரம்புகள் மற்றும் ஆபத்துகள், குறிப்பாக அவை அதிக நம்பிக்கை அல்லது அதிகாரத்துடன் முதலீடு செய்யப்படும்போது, ​​மேலும் அவர்களின் பொதுப் பாத்திரத்தில் பணிவு மற்றும் எச்சரிக்கையின் அவசியத்தையும் இது விவாதிக்கிறது. கூடுதலாக, புத்தகம் பொதுத் துறையில் தத்துவம் எதிர்கொள்ளும் அபாயங்கள் மற்றும் சவால்கள் மற்றும் நமது எதிர்காலத்தை வடிவமைக்க AIக்கான சாத்தியக்கூறுகள் ஆகியவற்றைத் தொடுகிறது."
      ],
      "metadata": {
        "id": "0_74CACi8ROX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing in other languages"
      ],
      "metadata": {
        "id": "X0GzMEvx8Zsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hindi -\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "translator = GoogleTranslator(source='auto', target='hi')\n",
        "translation = translator.translate(answer)\n",
        "print(translation)"
      ],
      "metadata": {
        "id": "JtMw0Le-pBYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741775221,
          "user_tz": -330,
          "elapsed": 6,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "ef07c31b-5dbe-4aa5-bb47-375b9b9c989b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "पुस्तक में जिन प्रमुख विशेषताओं पर प्रकाश डाला गया है, वे हैं एआई तकनीक की तीव्र प्रगति, समाज, संस्कृति और मूल्यों पर एआई का संभावित प्रभाव और मनुष्यों और एआई के बीच संबंध। पुस्तक में एआई की सीमाओं और खतरों तथा इसकी सार्वजनिक भूमिका में सावधानी और विनम्रता की आवश्यकता पर भी चर्चा की गई है। इसके अतिरिक्त, पुस्तक एआई-संचालित भविष्य को समझने और उसमें आगे बढ़ने में दर्शन की भूमिका का पता लगाती है।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# french\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "translator = GoogleTranslator(source='auto', target='fr')\n",
        "translation = translator.translate(answer)\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7B3RsP58gyb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741776128,
          "user_tz": -330,
          "elapsed": 910,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "ac3fb731-d2d9-4e20-9cfd-4fe994049ee0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les principales caractéristiques mises en évidence dans le livre sont les progrès rapides de la technologie de l’IA, l’impact potentiel de l’IA sur la société, la culture et les valeurs, ainsi que la relation entre les humains et l’IA. Le livre aborde également les limites et les dangers de l’IA, ainsi que la nécessité de faire preuve de prudence et d’humilité dans son rôle public. De plus, le livre explore le rôle de la philosophie dans la compréhension et la navigation dans l’avenir axé sur l’IA.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# german\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "translator = GoogleTranslator(source='auto', target='de')\n",
        "translation = translator.translate(answer)\n",
        "print(translation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iRbpdI88kLa",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741776998,
          "user_tz": -330,
          "elapsed": 872,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "0c7a377c-b141-4404-8f51-c6e77184f56c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die wichtigsten Aspekte, die das Buch hervorhebt, sind der schnelle Fortschritt der KI-Technologie, die möglichen Auswirkungen der KI auf Gesellschaft, Kultur und Werte sowie die Beziehung zwischen Mensch und KI. Das Buch diskutiert auch die Grenzen und Gefahren der KI und die Notwendigkeit von Vorsicht und Bescheidenheit in ihrer öffentlichen Rolle. Darüber hinaus untersucht das Buch die Rolle der Philosophie beim Verständnis und der Bewältigung der KI-gesteuerten Zukunft.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# question in tamil -> response in english\n",
        "question = \"சமூக ஊடகங்களில் gpt என்ன பங்கு வகிக்கிறது\"\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "translation = translator.translate(question)\n",
        "answer_en = rqa.invoke(translation)[\"result\"]\n",
        "print(answer_en)\n",
        "translator2 = GoogleTranslator(source = \"auto\", target = \"ta\")\n",
        "answer_tamil = translator2.translate(answer_en)\n",
        "print(answer_tamil)\n"
      ],
      "metadata": {
        "id": "w2M81DkY8mjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741778078,
          "user_tz": -330,
          "elapsed": 1082,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "3528e698-ab07-4f32-8f3b-350615191855"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " GPT can play a role in social media by generating content, such as titles for videos, headlines for articles, or responses to user queries. It can also be fine-tuned on a user's own data to generate personalized and consistent content that reflects their personality and preferences. However, it's important to note that GPT is a tool and its outputs should be carefully monitored and evaluated, and the use of GPT should be disclosed to the audience.\n",
            "வீடியோக்களுக்கான தலைப்புகள், கட்டுரைகளுக்கான தலைப்புச் செய்திகள் அல்லது பயனர் கேள்விகளுக்கான பதில்கள் போன்ற உள்ளடக்கத்தை உருவாக்குவதன் மூலம் GPT சமூக ஊடகங்களில் ஒரு பங்கை வகிக்க முடியும். இது ஒரு பயனரின் சொந்தத் தரவில் அவர்களின் ஆளுமை மற்றும் விருப்பங்களைப் பிரதிபலிக்கும் தனிப்பயனாக்கப்பட்ட மற்றும் நிலையான உள்ளடக்கத்தை உருவாக்கும் இருப்பினும், GPT என்பது ஒரு கருவி மற்றும் அதன் வெளியீடுகள் கவனமாக கண்காணிக்கப்பட்டு மதிப்பீடு செய்யப்பட வேண்டும், மேலும் GPT இன் பயன்பாடு பார்வையாளர்களுக்கு வெளிப்படுத்தப்பட வேண்டும் என்பதைக் கவனத்தில் கொள்ள வேண்டும்.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language detection and answer generation\n"
      ],
      "metadata": {
        "id": "fmcNKvGMhLFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect"
      ],
      "metadata": {
        "id": "Al804hPZvBvW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741778078,
          "user_tz": -330,
          "elapsed": 4,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# conversation in tamil\n",
        "question = input(\"enter the question - \")\n",
        "lang = detect(question)\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "translation = translator.translate(question)\n",
        "answer_en = rqa.invoke(translation)[\"result\"]\n",
        "translator2 = GoogleTranslator(source = \"auto\", target = lang)\n",
        "answer_lang = translator2.translate(answer_en)\n",
        "print(\"answer - \")\n",
        "print(answer_lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AES_-exCgf87",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721739091063,
          "user_tz": -330,
          "elapsed": 50967,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "2efd1eb3-40c7-4f71-f899-9415085f8e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the question சமூக ஊடகங்களில் AI இன் தாக்கம் என்ன?\n",
            "answer - \n",
            "சமூக ஊடகங்களில் AI இன் தாக்கம் குறிப்பிடத்தக்கது. இது உள்ளடக்கத்தை கட்டுப்படுத்தவும், உள்ளடக்கம் மற்றும் தயாரிப்பு பரிந்துரைகளை வடிவமைக்கவும், படைப்பாளர்களின் உற்பத்தி ஆற்றலை அதிகரிக்கவும் உதவுகிறது. எவ்வாறாயினும், சமூக ஊடகங்கள் நம்பகத்தன்மை, உடனடித் தன்மை மற்றும் மனிதத் தொடுதல் ஆகியவற்றை மதிப்பதால் இது ஒரு முரண்பாட்டை முன்வைக்கிறது. AI ஆனது போலியான மற்றும் வடிகட்டி குமிழ்கள் மற்றும் எதிரொலி அறைகளை உருவாக்க உதவுகிறது. தொழில்துறையின் போக்குகள், வேலை முறைகள் மற்றும் வாழ்க்கைப் பாதைகள் ஆகியவற்றில் AI தொடர்ந்து ஆழமான தாக்கத்தை ஏற்படுத்தும் என்று எதிர்பார்க்கப்படுகிறது, மேலும் இப்போது அதில் திறன்கள் மற்றும் திறன்களை வளர்த்துக்கொள்வது வரவிருக்கும் ஆண்டுகளில் பலன்களைத் தரும்.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#conversation in spanish\n",
        "question = input(\"enter the question - \")\n",
        "lang = detect(question)\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "translation = translator.translate(question)\n",
        "answer_en = rqa.invoke(translation)[\"result\"]\n",
        "translator2 = GoogleTranslator(source = \"auto\", target = lang)\n",
        "answer_lang = translator2.translate(answer_en)\n",
        "print(\"answer - \")\n",
        "print(answer_lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47zDoxl8g-w6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721739143889,
          "user_tz": -330,
          "elapsed": 52832,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "0205f7d4-0c4b-42a7-910f-850a28140c40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the question - ¿Cuál es el concepto principal destacado en el libro?\n",
            "answer - \n",
            "El concepto principal destacado en el libro es que las organizaciones y los gerentes están utilizando metáforas obsoletas para pensar sobre la relación laboral, y que se necesita un nuevo marco para acomodar mejor a los profesionales cada vez más emprendedores y empoderados en el lugar de trabajo moderno.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conversation in german -\n",
        "question = input(\"enter the question - \")\n",
        "lang = detect(question)\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "translation = translator.translate(question)\n",
        "answer_en = rqa.invoke(translation)[\"result\"]\n",
        "translator2 = GoogleTranslator(source = \"auto\", target = lang)\n",
        "answer_lang = translator2.translate(answer_en)\n",
        "print(\"answer - \")\n",
        "print(answer_lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIT3fPXYhtKh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721739156033,
          "user_tz": -330,
          "elapsed": 12149,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "67281b80-a435-443f-f336-5dd83d671b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the question - Was ist das Hauptkonzept, das in dem Buch hervorgehoben wird?\n",
            "answer - \n",
            "Das Hauptkonzept des Buches besteht darin, dass Organisationen und Manager veraltete Metaphern verwenden, um über das Arbeitsverhältnis nachzudenken, und dass ein neuer Rahmen erforderlich ist, um den zunehmend unternehmerisch denkenden und selbstbewussten Fachkräften am modernen Arbeitsplatz besser gerecht zu werden.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCR text as new context"
      ],
      "metadata": {
        "id": "NYZcZx2XiCfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we are going to feed the RAG model with the context derived by performing OCR on input images.\n"
      ],
      "metadata": {
        "id": "W0-jNV0pozP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OCR_doc_path = \"/content/drive/MyDrive/Colab Notebooks/OCRImages/Contextdoc.txt\"\n"
      ],
      "metadata": {
        "id": "tido5NdipK5K",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741859660,
          "user_tz": -330,
          "elapsed": 665,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the file -\n",
        "context_doc = \"\"\n",
        "with open(OCR_doc_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "    context_doc += content\n",
        "\n"
      ],
      "metadata": {
        "id": "8gPV5RwWp2Ne",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741861662,
          "user_tz": -330,
          "elapsed": 1283,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_doc[:200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "vWjO7_MQqhBy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741861662,
          "user_tz": -330,
          "elapsed": 5,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "3339f8f6-99e4-4130-f2f1-44d89c70c915"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A Review of Generative AI from Historical\\nPerspectives\\nAbstract—Many applications of Generative AI (such as DALL-\\nE, GPT-3, ChatGPT, etc.) are making headline news in recent\\nmonths and have been recei'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_split = text_splitter.split_text(context_doc)"
      ],
      "metadata": {
        "id": "unbrGUSUqj9G",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741865849,
          "user_tz": -330,
          "elapsed": 1108,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(context_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdUdzYEcrAVS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741865849,
          "user_tz": -330,
          "elapsed": 12,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "305febba-ce85-4752-9a24-f10a5198932d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(context_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMsqbSiQrDoj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741865849,
          "user_tz": -330,
          "elapsed": 12,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "70bbd122-7794-4e57-a493-d9d5396416a4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-small-en\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "wnKyl-ITrFxG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741867791,
          "user_tz": -330,
          "elapsed": 1946,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_search1 = FAISS.from_texts(context_split, embeddings)"
      ],
      "metadata": {
        "id": "MEmR1BaBrX6x",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741879262,
          "user_tz": -330,
          "elapsed": 11474,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_search1.embedding_function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqRUr7TWrhCN",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741879262,
          "user_tz": -330,
          "elapsed": 4,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "b5e6d9d8-6b45-42ef-b163-7b4e65bfbada"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              "  (2): Normalize()\n",
              "), model_name='BAAI/bge-small-en', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='', show_progress=False)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the title of this book ?\"\n",
        "docs = doc_search1.similarity_search(query)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8lpEuz1r7jh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741922087,
          "user_tz": -330,
          "elapsed": 4,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "1b430d01-bcbc-4e41-ae84-354f41297c71"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='a much smaller set of labeled examples. However, interestingly\\nGPT-3 does not use fine-tuning. In particular, it is seen that\\nfine-tuning hurts performance when we want to generalize to\\nout-of-distribution test data. The goal of GPT-3 is to be task-\\nagnostic and generalize to few-shot, one-shot and zero-shot\\ncases.\\nIn the few-shot case, the model is shown a few demon-\\nstrations of an NLP task and then given a novel instance\\nto solve. For example, K instances of translation from one\\nlanguage to another are shown to the model and the model\\nhas to translate a new instance. In the one-shot case, a natural\\nlanguage description of the task is provided along with a single\\ndemonstration. This is also closely related to how humans\\nlearn, i.e., without requiring many cases for the same task.\\nIn zero-shot, no examples are shown and only the natural\\nlanguage description of the task is given to the model. The\\nmodel is then expected to solve the task for a new instance.'),\n",
              " Document(page_content='In zero-shot, no examples are shown and only the natural\\nlanguage description of the task is given to the model. The\\nmodel is then expected to solve the task for a new instance.\\nArguably, this is the hardest and almost equivalent to how\\nhumans are expected to perform tasks. schematic view of GPT-\\n3 is shown in Fig.[I] It has been shown that in all these settings\\nGPT-3 (and later versions used in ChatGPT) have performed\\nexceptionally well, thus pushing us closer towards general in-\\ntelligence. In particular, the success of ChatGPT in generating\\nresponses from an initial prompt where the quality of text\\ngenerated is virtually indistinguishable from human-generated\\nresponses has significant technological, social and ethical\\nimplications . Next, we present a conceptual pipeline for\\ngenerative AI applications.\\nD. Generative AI Pipeline\\n1) Preprocessing: Inputs to a generative AI pipeline may\\nbe homogeneous (a single type) or heterogeneous (a mixture of'),\n",
              " Document(page_content='is the value matrix and d; is the dimensionality of the em-\\nbedding that represents the latent representations. The query,\\nkey and value matrices are abstractions, where query can be\\nviewed as an “input” (tokens that are seeking the attention),\\nand (key,value) pairs refer to the “output” (which token has\\ninformation about the attention) and how much attention each\\ntoken has is denoted by the value which is encoded in the form\\nof attention weights. The transformer architecture computes\\nmultiple attentions in parallel using multiple heads as given\\nby the following equation.\\nMultiHead(Q, K,V) = Concat(head,,..., headn)W?\\n(3)\\nwhere head; =  Attention(QW2,KWK,VW) and\\nW2, WE, WY are projection matrices, and W° is a weight\\nmatrix.C. Pre-training Generative Models\\nA key step that is critical to the remarkable performance\\nof GPT is generative pre-trained transformer. Specifically,\\nthe transformer model is pre-trained over extremely large'),\n",
              " Document(page_content='to produce human-like answers to questions. At the same time,\\nseveral questions remain about misinformation/disinformation\\ndvenugopal @memphis.edu\\nthat may be spread through the use of these models and the\\nfar-reaching impact of these models on future student learning\\nand development.\\nAll these technological transformations are possible due\\nto hardware/software advances (such as massive computation\\npower and exponential growth in storage capabilities). While\\nAl-based tools are showing significant benefits, there is also an\\ninherent danger that several so-called AI industry experts may\\nuse these powerful technologies as a blackbox without deep\\nunderstanding of how they work and their inherent limitations.\\nII. LESSONS FROM THE PAST\\nSearch and optimization are key to all AI algorithms,\\nthese use different similarity measures (pattern matching) to\\nprovide guided search in representation or problem space. To\\naccomplish proper guiding, selection schemes, fitness function,')]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retriever = doc_search1.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":4})\n",
        "\n",
        "ocr_rqa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "21B10RN6tU_i",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721741933656,
          "user_tz": -330,
          "elapsed": 668,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        }
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = input(\"enter the question - \")\n",
        "lang = detect(question)\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "translation = translator.translate(question)\n",
        "answer_en = ocr_rqa.invoke(translation)[\"result\"]\n",
        "translator2 = GoogleTranslator(source = \"auto\", target = lang)\n",
        "answer_lang = translator2.translate(answer_en)\n",
        "print(\"answer - \")\n",
        "print(answer_lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY1EsQERsU20",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721742186328,
          "user_tz": -330,
          "elapsed": 19355,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "5d1b2a7a-705a-4abc-fcaa-c8e31f12a90e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the question - what are the pipeline stages involved in generative ai ?\n",
            "answer - \n",
            "The pipeline stages involved in generative AI, as described in the context, are:\n",
            "\n",
            "1. Preprocessing: This stage involves preparing the inputs to a generative AI pipeline. The inputs can be homogeneous (a single type) or heterogeneous (a mixture of several types). Depending on the type of unstructured data, one or more steps of pre-processing may be needed.\n",
            "\n",
            "2. Extract, Transform, and Load (ETL): This is a common term used for pre-processing of large datasets. It involves extracting data from various sources, transforming it into a suitable format, and loading it into the system.\n",
            "\n",
            "3. Tokenization: This step involves breaking down the data into smaller pieces, called tokens, which can be processed by the AI model. For text data, this might involve using speech-to-text tools or tokenizers like NLTK, Tensorflow Text, or HuggingFace tokenizers.\n",
            "\n",
            "4. Reading Comprehension: This step involves understanding the context and meaning of the data.\n",
            "\n",
            "5. Downstream Language Tasks: These are tasks that the AI model performs after understanding the data. Examples include text completion, visual question answering, and text to image generation.\n",
            "\n",
            "6. Downstream Multimodal Tasks: These are tasks that involve multiple types of data. An example given is text to image generation.\n",
            "\n",
            "7. Inference: This step involves the AI model making predictions or decisions based on the data it has processed. This can be done in various settings, such as few-shot, one-shot, or zero-shot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = input(\"enter the question - \")\n",
        "lang = detect(question)\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "translation = translator.translate(question)\n",
        "answer_en = ocr_rqa.invoke(translation)[\"result\"]\n",
        "translator2 = GoogleTranslator(source = \"auto\", target = lang)\n",
        "answer_lang = translator2.translate(answer_en)\n",
        "print(\"answer - \")\n",
        "print(answer_lang)"
      ],
      "metadata": {
        "id": "Df2__R_9uVla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721742348815,
          "user_tz": -330,
          "elapsed": 22725,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "89f21d3f-49cb-47c0-bbc3-a83b1ffd4c92"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the question - what is meant by deepfake ?\n",
            "answer - \n",
            "Deepfake refers to the use of artificial intelligence (AI) to create realistic videos or images that impersonate someone, such as by swapping their facial expressions with those of someone being tracked using a depth-sensing camera, or by generating realistic videos or images that didn't actually happen. This technology can be used for various purposes, some of which may be controversial or used to spread misinformation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conversation in tamil\n",
        "question = input(\"enter the question - \")\n",
        "lang = detect(question)\n",
        "translator = GoogleTranslator(source='auto', target='en')\n",
        "translation = translator.translate(question)\n",
        "answer_en = ocr_rqa.invoke(translation)[\"result\"]\n",
        "translator2 = GoogleTranslator(source = \"auto\", target = lang)\n",
        "answer_lang = translator2.translate(answer_en)\n",
        "print(\"answer - \")\n",
        "print(answer_lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw7Y1EPU1bKA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1721742490194,
          "user_tz": -330,
          "elapsed": 38993,
          "user": {
            "displayName": "dark knight",
            "userId": "13559104322727335975"
          }
        },
        "outputId": "6deb2e82-4d33-4b72-f4ef-4b1c51b42016"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the question - பேச்சு தொகுப்பு என்றால் என்ன?\n",
            "answer - \n",
            "பேச்சு தொகுப்பு என்பது இயந்திர கற்றல் நுட்பங்களைப் பயன்படுத்தி ஒரு குறிப்பிட்ட நபரின் குரலைப் பிரதிபலிக்கும் பேச்சை உருவாக்கும் செயல்முறையாகும். அல்-அடிப்படையிலான பேச்சுத் தொகுப்பின் ஒரு எடுத்துக்காட்டு லைரிபேர்டாட் இயங்குதளமாகும், இது ஒரு தனிநபரின் பேச்சின் மாதிரியை பகுப்பாய்வு செய்வதன் மூலம் அவர்களின் பேச்சு முறைகள் மற்றும் பண்புகளை மாதிரியாக்குகிறது மற்றும் அந்த நபரின் குரலில் புதிய பேச்சை உருவாக்க முடியும்.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nZ9LAB3-2NiP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
